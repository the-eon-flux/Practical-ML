---
title: "Combining Predictors"
author: "Tejus"
date: "15/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR); data(Wage); library(ggplot2); library(caret);
```


### Key ideas

* You can combine different classifiers by averaging/voting (Boosting classfier + RF + lm)
* Combining classifiers improves accuracy but reduces interpretability
* Boosting, bagging, and random forests are variants on this theme with same classfier used

### Basic intuition - majority vote

Suppose we have 5 completely independent classifiers

If accuracy is 70% for each:

  * $10\times(0.7)^3(0.3)^2 + 5\times(0.7)^4(0.3)^2 + (0.7)^5$
  * 83.7% majority vote accuracy

With 101 independent classifiers
  * 99.9% majority vote accuracy
  

### Approaches for combining classifiers

1. Bagging, boosting, random forests
  * Usually combine similar classifiers
2. Combining different classifiers
  * Model stacking
  * Model ensembling

### Model Stacking 


* Example with Wage data

__Create training, test and validation sets__

```{r wage, cache=TRUE, warning=FALSE}

Wage <- subset(Wage,select=-c(logwage))
# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]

inTrain <- createDataPartition(y=buildData$wage,
                              p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]

## Build two different models
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
              data=training, 
              trControl = trainControl(method="cv"),number=3)

## Predict on the testing set 
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
```

* Neither of them perfectly correlate to the `Wage` variable. Also they're not perfectly on the diagonal

#### Fit a model that combines predictors

```{r combine, cache=TRUE, warning=FALSE}
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
```


* Reduced the testing error

```{r ErrorsTesting}
sqrt(sum((pred1-testing$wage)^2)) # model 1
sqrt(sum((pred2-testing$wage)^2)) # model 2
sqrt(sum((combPred-testing$wage)^2)) # model 1 + 2
```

#### Predict on validation data set

```{r validation}
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
```

* Reduced the validation error

```{r ,dependson="validation"}
sqrt(sum((pred1V-validation$wage)^2)) # model 1
sqrt(sum((pred2V-validation$wage)^2)) # model 2
sqrt(sum((combPredV-validation$wage)^2)) # model 1 + 2
```


## Notes and further resources

* Even simple blending can be useful
* Typical model for binary/multiclass data
  * Build an odd number of models
  * Predict with each model
  * Predict the class by majority vote
* This can get dramatically more complicated
  * Simple blending in caret: [caretEnsemble](https://github.com/zachmayer/caretEnsemble) (use at your own risk!)
  * Wikipedia [ensemlbe learning](http://en.wikipedia.org/wiki/Ensemble_learning)
