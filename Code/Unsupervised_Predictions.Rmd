---
title: "Unsupervised Prediction"
author: "Tejus"
date: "16/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2); library(caret)
```

### Key ideas

* Sometimes you don't know the labels for prediction
* To build a predictor
  * Create clusters (Not such an easy & noiseless problem)
  * Name clusters (Challenging also)
  * Build predictor for clusters
* In a new data set
  * Predict clusters

### Iris example ignoring species labels

```{r iris}
data(iris); 
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
```


### Cluster with k-means

```{r kmeans,fig.height=4,fig.width=6}
kMeans1 <- kmeans(subset(training,select=-c(Species)),centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)

# Real values 
table(kMeans1$cluster,training$Species)

```

### Build Predictor

```{r Buildmodel, cache=TRUE}
modFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method="rpart")
table(predict(modFit,training),training$Species)

# For test
testClusterPred <- predict(modFit,testing) 
table(testClusterPred ,testing$Species)
```

## Notes and further reading

* The cl_predict function in the clue package provides similar functionality
* Beware over-interpretation of clusters!
* This is one basic approach to [recommendation engines](http://en.wikipedia.org/wiki/Recommender_system)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
