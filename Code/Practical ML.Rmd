---
title: "Practical ML"
author: "the-eon-flux"
date: "26/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(kernlab)
data(spam)
```

## Machine learning algorithms in R

* Linear discriminant analysis
* Regression
* Naive Bayes
* Support vector machines
* Classification and regression trees
* Random forests
* Boosting
* etc. 

***

## `caret` package functionalities

* Some preprocessing (cleaning)
  * preProcess
* Data splitting
  * createDataPartition
  * createResample
  * createTimeSlices
* Training/testing functions
  * train
  * predict
* Model comparison
  * confusionMatrix

***

### Data Splitting

* `createDataPartition()` : allows you to split the data easily

```{r spam, echo= TRUE, cache=TRUE}
dim(spam)

inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
        # Train the model with 75% data; p = 0.75

training <- spam[inTrain,]
testing <- spam[-inTrain,]

dim(testing)
dim(training)
```

### Fit a model

* `train()` : To fit a model

```{r Train, cache=TRUE, warning=FALSE}
set.seed(32343)
modelFit <- train(type ~. , data = training, method = "glm")
modelFit # Consice summary
```

* Best model from the bootstraps

```{r Train_1}
# Best model after bootstraping
modelFit$finalModel

```


### Predictions

```{r Prediction, cache=TRUE}

predictions <- predict(modelFit, newdata = testing) # note for testing data we're predicting
head(predictions)

```

### Confusion Matrix

* Shows a contingency table & other summary statistics

```{r Model_Comparisons, cache=TRUE}

Results <- confusionMatrix(predictions, testing$type) # data = data used for prediction
Results
```


## Cross-validation 

* Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. 

* _Approach_:

  1. Use the training set
  
  2. Split it into training/test sets 
  
  3. Build a model on the training set
  
  4. Evaluate on the test set
  
  5. Repeat and average the estimated errors

* _Used for_:

  1. Picking variables to include in a model
  
  2. Picking the type of prediction function to use
  
  3. Picking the parameters in the prediction function
  
  4. Comparing different predictors

### Cross validation methods : K-fold cross-validation

* **k-fold sampling** is a which procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.

* The general procedure is as follows:

* Shuffle the dataset randomly.
* Split the dataset into k groups
* For each unique group:
    1. Take the group as a hold out or test data set
    2. Take the remaining groups as a training data set
    3. Fit a model on the training set and evaluate it on the test set
    4. Retain the evaluation score and discard the model
    
* Summarize the skill of the model using the sample of model evaluation scores.

* Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.

```{r k_fold}

set.seed(73)

# Training set returned
folds <- createFolds(y = spam$type, k =10, list = TRUE, returnTrain = TRUE)
sapply(folds, length)

```

* For test set

```{r k_fold2, cache=TRUE}

set.seed(73)

# Testing set returned
folds <- createFolds(y = spam$type, k =10, list = TRUE, returnTrain = FALSE)
sapply(folds, length)

```

* Always remember for k-fold cross validation
  * Larger k = less bias, more variance
  * Smaller k = more bias, less variance


### Cross validation methods : Resampling

* `Resampling` method with replacement

```{r Resampling, cache=TRUE}
set.seed(32323)
folds <- createResample(y = spam$type, times = 10, list = TRUE)
sapply(folds, length)

```

* Random sampling with replacement is the _bootstrap_
  * Underestimates of the error
  * Can be corrected, but it is complicated ([0.632 Bootstrap](http://www.jstor.org/discover/10.2307/2965703?uid=2&uid=4&sid=21103054448997))
  
### Cross validation methods : Timeslices resampling

* For time series data, data must be used in "chunks". Chunk size is set by below 2 params `initialWindow & horizon`.

* The param `initialWindow` is for defining the initial number of consecutive values in each training set sample.

* The param `horizon` is for defining the initial number of consecutive values in each testing set sample.

```{r Time Slices,cache=TRUE}
time <- 1:1000
folds <- createTimeSlices(y = time, initialWindow = 20, horizon = 10)

folds$train[[1]] # Gives training slices

folds$test[[1]] # Test slices

```


## Additional training methods : 

* `preProcess` param : Set preprocessing options

* `weights` param : Give weightage (for eg when samples of 1 class are more)

```{r Training_opts, eval=FALSE}

# Default args for train()
function(x, y,
            method = "rf",
            preProcess = NULL,
            ...,
            weights = NULL,
            metric = ifelse(is.factor(y), "Accuracy", "RMSE"),
            maximize = ifelse(metric %in% c("RMSE", "logLoss", "MAE"), FALSE, TRUE),
            trControl = trainControl(),
            tuneGrid = NULL,
            tuneLength = ifelse(trControl$method == "none", 1, 3))

```

* `metric` param : Options

      * Continous outcomes
            * RMSE = Root mean squared error
            * RSquared = $R^2$ from Regression models
      * Categorcial outcomes
            * Accuracy = Fraction of correct
            * Kappa = Measure of concordance

* trainControl (`trControl`) param arguments give you detailed parameter setting options to train models

      * `number` = Bootstrapping count
      * `p` = size of training set
      * `seeds` = Setting seed while doing parallel processing
      * `repeat` = Number of times to repeat subsampling

```{r eval=FALSE}

# args(trainControl)

function (method = "boot", number = ifelse(grepl("cv", 
    method), 10, 25), repeats = ifelse(grepl("[d_]cv$", 
    method), 1, NA), p = 0.75, search = "grid", initialWindow = NULL, 
    horizon = 1, fixedWindow = TRUE, skip = 0, verboseIter = FALSE, 
    returnData = TRUE, returnResamp = "final", savePredictions = FALSE, 
    classProbs = FALSE, summaryFunction = defaultSummary, selectionFunction = "best", 
    preProcOptions = list(thresh = 0.95, ICAcomp = 3, k = 5, 
        freqCut = 95/5, uniqueCut = 10, cutoff = 0.9), sampling = NULL, 
    index = NULL, indexOut = NULL, indexFinal = NULL, timingSamps = 0, 
    predictionBounds = rep(FALSE, 2), seeds = NA, adaptive = list(min = 5, 
        alpha = 0.05, method = "gls", complete = TRUE), 
    trim = FALSE, allowParallel = TRUE)

```













